hidden-layer dimensions的取值取决于拟合程度，这也取决于离线学习的网络规模大小，避免过拟合与过慢训练，稿件中在16节点网络取值为1024，72节点网络取值为2048，225节点网络取值为5120；dropout rate是 Dropout 正则化技术的核心参数，其目的是通过随机“破坏”网络结构来防止过拟合，副作用是，它会降低模型的学习效率（因为每次只更新一部分参数）和有效容量，其常用的取值范围为[0.1,0.5]，稿件中取值为0.2；optimizer learning rate是决定优化器在参数空间中每一步的步长，过大的取值会导致无法收敛，过小的取值会导致收敛过慢，其取值也跟网络的规模有关联，常用取值范围为[0.000001,0.1]，稿件中取值为0.0001；batch size的设定对训练速度、模型稳定性和最终性能都有显著影响，当其取值较大时，能更充分地利用硬件（尤其是GPU）的并行计算能力，加快训练速度，同时提供更稳定的梯度估计，而较小的Batch Size通常能为模型带来更好的泛化性能，其取值可以通过实验和验证集进行确定，常用取值范围为32,64,128,256,512,1024,稿件中取值为32；epochs是一个与其他超参数性质截然不同的超参数，它的取值原则更侧重于终止条件而非预先设定，并且其“最佳值”完全由模型在数据上的表现动态决定，依赖于数据集的大小和模型的复杂度，稿件中在16节点网络中取值为50，72节点网络中取值为100，在225节点网络中取值为200。